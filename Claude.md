# Claude Development Guide

## Project Overview

A minimal, local-first ML application that predicts developer salaries using Stack Overflow
Developer Survey data. Built with Python 3.12, XGBoost, Pydantic v2, and Streamlit. Emphasises
clarity and simplicity over production completeness.

## Tech Stack

- **Python 3.12+**
- **uv** — Package & virtual environment management
- **pandas** — Data manipulation
- **xgboost** — Gradient boosting model (primary model)
- **scikit-learn** — Cross-validation and train/test split
- **optuna** — Hyperparameter optimisation
- **pydantic** — Input schema validation (v2)
- **streamlit** — Web UI
- **ruff** — Linting and formatting
- **radon** — Cyclomatic complexity and maintainability metrics
- **bandit** — Static security analysis
- **pip-audit** — Dependency vulnerability scanning
- **pre-commit** — Git hook management

## Project Structure

```text
.
├── .github/
│   └── workflows/
│       └── ci.yml                   # GitHub Actions CI (lint + test on every push)
├── config/
│   ├── model_parameters.yaml        # Model config, guardrails, cardinality settings
│   ├── optuna_config.yaml           # Optuna search space and trial settings
│   ├── valid_categories.yaml        # Valid input categories (generated by training)
│   └── currency_rates.yaml          # Per-country currency rates (generated by training)
├── data/
│   └── survey_results_public.csv    # Stack Overflow survey data (download required)
├── models/
│   └── model.pkl                    # Trained model artifact (generated by training)
├── src/
│   ├── __init__.py
│   ├── schema.py                    # Pydantic input model (SalaryInput)
│   ├── preprocessing.py             # Feature engineering (one-hot encoding, scaling)
│   ├── train.py                     # Training pipeline
│   ├── tune.py                      # Optuna hyperparameter optimisation
│   └── infer.py                     # Inference with runtime category guardrails
├── tests/
│   ├── conftest.py                  # Shared pytest fixtures
│   ├── test_schema.py               # Pydantic validation tests
│   ├── test_infer.py                # Inference and guardrail tests
│   ├── test_train.py                # Training pipeline helper tests
│   ├── test_preprocessing.py        # Feature engineering tests
│   ├── test_tune.py                 # Optuna tuning tests
│   └── test_feature_impact.py       # Model sanity — each feature affects predictions
├── app.py                           # Streamlit web app
├── example_inference.py             # Programmatic usage examples
├── Makefile                         # Developer workflow commands
├── .pre-commit-config.yaml          # Pre-commit hooks (format, lint, standard checks)
├── pyproject.toml                   # Project metadata and dependencies (uv)
└── README.md                        # Project documentation + HuggingFace Space config
```

## Setup & Installation

```bash
# Install dependencies
uv sync

# Install pre-commit hooks (once, after cloning)
uv run pre-commit install
```

## Key Workflows

All common tasks are available via the Makefile. Run `make help` to list targets.

### Full quality check

```bash
make check   # lint + test + complexity + maintainability + audit + security
```

### Individual targets

| Target | What it does |
| ------ | ------------ |
| `make lint` | ruff check (style + errors) |
| `make format` | ruff format (auto-format) |
| `make test` | pytest — all tests |
| `make coverage` | pytest with HTML coverage report |
| `make complexity` | radon cyclomatic complexity |
| `make maintainability` | radon maintainability index |
| `make audit` | pip-audit dependency vulnerability scan |
| `make security` | bandit static security analysis |
| `make pre-process` | Validate data + generate config artifacts (no model) |
| `make tune` | Optuna hyperparameter search |
| `make ci` | Mirror of GitHub Actions CI (lint + test) |
| `make pre-commit` | Run all pre-commit hooks against every file |

### Training the model

```bash
uv run python -m src.train
```

Generates:

- `models/model.pkl` — trained XGBoost model
- `config/valid_categories.yaml` — valid input values for runtime guardrails
- `config/currency_rates.yaml` — per-country median currency conversion rates

### Hyperparameter tuning (optional, run before training)

```bash
make tune
# or
uv run python -m src.tune --n-trials 50
```

Reads search space from `config/optuna_config.yaml`, writes best parameters back into
`config/model_parameters.yaml`.

### Running the Streamlit app

```bash
uv run streamlit run app.py
```

### Running inference programmatically

```python
from src.schema import SalaryInput
from src.infer import predict_salary

input_data = SalaryInput(
    country="United States of America",
    years_code=5.0,
    work_exp=3.0,
    education_level="Bachelor's degree (B.A., B.S., B.Eng., etc.)",
    dev_type="Developer, full-stack",
    industry="Software Development",
    age="25-34 years old",
    ic_or_pm="Individual contributor",
    org_size="20 to 99 employees",
    employment="Employed",
)
salary = predict_salary(input_data)
```

Valid values for each categorical field are listed in `config/valid_categories.yaml`
(generated at training time).

## Data Requirements

The `survey_results_public.csv` must include these columns:

| Column | Description |
| ------ | ----------- |
| `Country` | Developer's country of residence |
| `YearsCode` | Total years coding (including education) |
| `WorkExp` | Years of professional work experience |
| `EdLevel` | Highest education level |
| `DevType` | Primary developer role |
| `Industry` | Industry the developer works in |
| `Age` | Age range |
| `ICorPM` | Individual contributor or people manager |
| `OrgSize` | Organisation size (number of employees) |
| `Employment` | Current employment status |
| `ConvertedCompYearly` | Annual salary in USD (target variable) |

## Input Validation (Two Layers)

### Layer 1 — Pydantic schema (`src/schema.py`)

All 10 fields are required. `years_code` and `work_exp` must be `>= 0`. Validated at
object construction time — raises `ValidationError` on failure.

### Layer 2 — Runtime guardrails (`src/infer.py`)

Each categorical field is checked against `config/valid_categories.yaml` at inference
time. Raises `ValueError` with a clear message on invalid input.

## Key Files

### [src/schema.py](src/schema.py)

Pydantic v2 `SalaryInput` model — defines all 10 required input fields, types, and
constraints. The JSON schema example in the docstring is the canonical usage example.

### [src/preprocessing.py](src/preprocessing.py)

`prepare_features(df)` — takes a raw DataFrame and returns an encoded feature matrix:

- Unicode apostrophe normalisation on all categorical columns
- Rare category → "Other" normalisation
- Missing numeric values → 0
- Missing categoricals → "Unknown"
- One-hot encoding (training uses `drop_first=True`; inference uses `drop_first=False`
  then reindexes to match training columns)

### [src/train.py](src/train.py)

Full training pipeline:

1. Load CSV, filter salaries (min/max percentile per country)
2. `apply_cardinality_reduction` — collapse rare categories to "Other"
3. `drop_other_rows` — remove rows with "Other" in specified columns
4. `prepare_features` — encode features
5. 5-fold CV with MAPE metric
6. Train final XGBoost model on full data with early stopping
7. Save `models/model.pkl`, `config/valid_categories.yaml`, `config/currency_rates.yaml`

### [src/tune.py](src/tune.py)

Optuna study over the search space in `config/optuna_config.yaml`. Writes best
parameters back into `config/model_parameters.yaml` after the study completes.

### [src/infer.py](src/infer.py)

`predict_salary(SalaryInput)` — validates categories, builds a single-row DataFrame,
runs `prepare_features`, reindexes to training columns, returns float USD salary.
`get_local_currency(country, salary)` — converts to local currency using rates from
`config/currency_rates.yaml`.

### [app.py](app.py)

Streamlit UI with dropdowns populated from `config/valid_categories.yaml` (only values
that appeared in the training data). Shows USD + local currency side by side.

## Updating Features

When adding a new input feature, update **all** of the following in order:

1. `config/model_parameters.yaml` — add to `features.cardinality.drop_other_from` if applicable
2. `src/schema.py` — add field to `SalaryInput`
3. `src/preprocessing.py` — add to `_categorical_cols` (or numeric handling)
4. `src/train.py` — add to `CATEGORICAL_FEATURES` and `usecols`
5. `src/tune.py` — add to `usecols`
6. `src/preprocess.py` — add to `REQUIRED_COLUMNS`
7. `src/infer.py` — add validation block and DataFrame column
8. `app.py` — add selectbox, default, sidebar entry, `SalaryInput` construction
9. `tests/conftest.py` — add to `sample_salary_input` fixture
10. `tests/test_schema.py` — assert field, add missing-field test
11. `tests/test_infer.py` — add invalid-value test
12. `tests/test_feature_impact.py` — add to all `base_input` dicts, add impact test
13. `tests/test_preprocessing.py` — add column to all `pd.DataFrame(...)` fixtures
14. `tests/test_train.py` — add column to `_make_salary_df` and all test DataFrames
15. `README.md` — required columns, valid categories list, code example
16. `Claude.md` — data requirements table, field counts, code example, version
17. `example_inference.py` — add to all `SalaryInput` calls
18. Retrain: `uv run python -m src.train`

## Versioning

Follows [Semantic Versioning](https://semver.org/):

- **MAJOR** — new required input field, incompatible model artifact, renamed API
- **MINOR** — new optional field, new supported country, new Makefile target
- **PATCH** — bug fix, model retrain with same schema, config tuning

Current version: `3.0.0` (added `Employment` required field).

Update `pyproject.toml` before tagging:

```bash
git tag v2.0.0
git push origin v2.0.0
```

## Code Style

- Line length: **79 characters** (enforced by ruff `E501`)
- Formatter: `ruff format` (run via `make format`)
- Linter: `ruff check` (run via `make lint`)
- No `# noqa` suppressions — fix the code instead

## Common Debugging

```bash
# Check data columns
head -1 data/survey_results_public.csv | tr ',' '\n'

# Verify model exists
ls -lh models/model.pkl

# Check valid categories after training
cat config/valid_categories.yaml

# Run a single test file
uv run pytest tests/test_infer.py -v

# Run pre-commit manually
uv run pre-commit run --all-files
```

## Troubleshooting

| Symptom | Fix |
| ------- | --- |
| `FileNotFoundError: model.pkl` | Run `uv run python -m src.train` |
| `FileNotFoundError: valid_categories.yaml` | Same — generated by training |
| `ValidationError` on `SalaryInput` | Check all 10 fields are present and non-negative numerics |
| `ValueError: Invalid ...` at inference | Value not in `config/valid_categories.yaml`; retrain or use a listed value |
| `E501` ruff errors | Lines > 79 chars — split strings, use variables, or wrap lists |
| Tests fail after adding a feature | Check the "Updating Features" checklist above |

## Additional Resources

- [README.md](README.md) — User-facing documentation and HuggingFace Space config
- [Stack Overflow Survey](https://insights.stackoverflow.com/survey) — Data source
- [semver.org](https://semver.org/) — Versioning reference
